{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing using BERT\n",
    "def preprocess_text(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids, attention_mask = encoding['input_ids'], encoding['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    embeddings = torch.mean(last_hidden_state, 1)  # Mean pooling\n",
    "    return embeddings.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load preprocessed and split datasets\n",
    "train_df = pd.read_csv(\"train_IFND.csv\")\n",
    "val_df = pd.read_csv(\"val_IFND.csv\")\n",
    "test_df = pd.read_csv(\"test_IFND.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply feature extraction to the datasets\n",
    "def extract_features_sequential(df):\n",
    "    text_embeddings = []\n",
    "    for _, row in df.iterrows():\n",
    "        text_embeddings.append(preprocess_text(row['Statement']))\n",
    "    df['Text_Embedding'] = text_embeddings\n",
    "    df['Combined_Feature'] = df['Text_Embedding']  # Only using text features\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = extract_features_sequential(train_df)\n",
    "val_df = extract_features_sequential(val_df)\n",
    "test_df = extract_features_sequential(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert combined features and labels to numpy arrays\n",
    "def prepare_dataset(df):\n",
    "    X = np.stack(df['Combined_Feature'].values)\n",
    "    y = df['Label'].apply(lambda x: 1 if x == 'TRUE' else 0).values\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_dataset(train_df)\n",
    "X_val, y_val = prepare_dataset(val_df)\n",
    "X_test, y_test = prepare_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and DataLoaders\n",
    "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = TextClassifier(input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_accuracy += (preds == labels).float().mean()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_accuracy = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_accuracy += (preds == labels).float().mean()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import torch\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to collect predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the appropriate device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get the index of the max log-probability, i.e., the predicted class\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # Append predictions and true labels to respective lists\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=['FALSE', 'TRUE'])\n",
    "\n",
    "# Function to colorize classification report\n",
    "def colorize_report(report):\n",
    "    report_lines = report.split('\\n')\n",
    "    new_report = []\n",
    "    for line in report_lines:\n",
    "        if line.strip():\n",
    "            if line.startswith('FALSE') or line.startswith('TRUE'):\n",
    "                new_report.append('\\033[1m' + line + '\\033[0m')\n",
    "            else:\n",
    "                new_report.append(line)\n",
    "    return '\\n'.join(new_report)\n",
    "\n",
    "# Colorize classification report\n",
    "colorized_report = colorize_report(class_report)\n",
    "\n",
    "# Print colorized classification report\n",
    "print(\"Classification Report:\")\n",
    "print(colorized_report)\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Define labels\n",
    "labels = ['FALSE', 'TRUE']\n",
    "\n",
    "# Create seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Adjust font size\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
